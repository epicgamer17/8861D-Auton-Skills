{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfAOKuTuCO56yx5/IqGuNj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epicgamer17/8861D-Auton-Skills/blob/main/connect4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://colab.research.google.com/drive/1y0IJsBA-Hvdun59r-vZiT7-RndVj6Dzj?usp=sharing"
      ],
      "metadata": {
        "id": "amD1Hke46EfP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import random\n",
        "num_cols = 7\n",
        "num_rows = 6\n",
        "turn = 0\n",
        "\n",
        "board = np.array([[0]*num_cols for i in range(num_rows)])\n",
        "\n",
        "def play_turn():\n",
        "    global num_cols, num_rows, turn, board\n",
        "    column_to_play = input(\"What column do you want to place your token (1-7)\")\n",
        "    column_to_play = int(column_to_play) - 1\n",
        "    if column_to_play < 0 or column_to_play > 6:\n",
        "        print(\"Invalid column\")\n",
        "        return False\n",
        "    if board[0][column_to_play] != 0:\n",
        "        print(\"Column full\")\n",
        "        return False\n",
        "\n",
        "    for t in range(num_rows-1, -1, -1):\n",
        "        if (board[t][column_to_play] == 0):\n",
        "            board[t][column_to_play] = (turn % 2) + 1\n",
        "            turn += 1\n",
        "            return True\n",
        "\n",
        "# https://stackoverflow.com/questions/74692439/how-do-i-check-for-a-4-in-a-row-with-a-2d-array-note-that-im-not-using-nump\n",
        "def check_victory(board: np.array) -> int:\n",
        "    def check_subfield(field: np.array) -> int:\n",
        "        def same_number(values: list) -> int:\n",
        "            return int((values == values[0]).all() * values[0])\n",
        "        # check diagonal.\n",
        "        v = same_number(np.diagonal(field))\n",
        "        if v > 0:\n",
        "            return v\n",
        "        for i in range(4):\n",
        "            v = same_number(field[i, :])\n",
        "            if v > 0:\n",
        "                return v\n",
        "            v = same_number(field[:, i])\n",
        "            if v > 0:\n",
        "                return v\n",
        "        return 0\n",
        "\n",
        "    for row in range((num_rows + 1) - 4):\n",
        "        for col in range((num_cols + 1) - 4):\n",
        "            v = check_subfield(board[row:row + 4, col:col + 4])\n",
        "            if v > 0:\n",
        "                return v\n",
        "    return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def play_game():\n",
        "    while turn < num_rows*num_cols:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        play_turn()\n",
        "        for r in range(num_rows):\n",
        "            print(board[r])\n",
        "        if (check_victory(board) != 0):\n",
        "            print(\"Winner was Player {}\".format(check_victory(board)))\n",
        "            return\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "012A3DXd4ngc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_play_turn():\n"
      ],
      "metadata": {
        "id": "KJZhEEhr4wmX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "50390b5d-9cd2-4809-9a26-77196a414dcc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-3-b5946e9c18d0>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b5946e9c18d0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def model_play_turn():\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play_game()\n"
      ],
      "metadata": {
        "id": "Sl56xGaH4v8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Environment Stuff\n",
        "\n",
        "#State Space\n",
        "state_space = num_cols * num_rows\n",
        "\n",
        "#Action Space\n",
        "action_space = num_cols # could make the action space alter based on the number of\n",
        "                                                        # available columns so we don't need to give a negative\n",
        "                                                        # reward for illegal actions?\n",
        "\n",
        "\n",
        "#Reward Table\n",
        "\n",
        "#etc...\n"
      ],
      "metadata": {
        "id": "uNzRnG-L4vSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Value Function\n",
        "# 1. Q-Learning\n",
        "# https://en.wikipedia.org/wiki/Q-learning\n",
        "# https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
        "\n",
        "\n",
        "q_table = np.zeroes((state_space, action_space))  # could make the action space alter based on the number of\n",
        "                                                        # available columns so we don't need to give a negative\n",
        "                                                        # reward for illegal actions?\n",
        "\n",
        "steps = 0\n",
        "discount_factor = 0.5   # A number between 0 and 1. If discount_factor is less\n",
        "                        # than 1 it values rewards received earlier as higher (gamma)\n",
        "learning_rate = 0.1     # a number between 0 and 1 (not 0) (alpha)\n",
        "expoloration_rate = 0.1 # a number between 0 and 1 (epsilon)\n",
        "\n",
        "#Train the agent\n",
        "num_epocs = 1000\n",
        "for i in range(num_epocs):\n",
        "    #reset state space\n",
        "    board = np.array([[0]*num_cols for i in range(num_rows)])\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < expoloration_rate:\n",
        "            #SELECT A RANDOM ACTION\n",
        "            action = random.randint(0, 6)\n",
        "        else:\n",
        "            #pick the best action in current state using the q-table\n",
        "            action = np.argmax(q_table[board])  # FIND A WAY TO TURN THE BOARD STATE\n",
        "                                                # INTO AN INT THAT REPRESENTS THE BOARD,\n",
        "                                                # OR FIND A WAY OF MAKING THE Q-TABLE\n",
        "                                                # WORK WHEN PASSING IN A 2-D ARRAY\n",
        "                                                # (MAYBE WITH A DICTIONARY?)\n",
        "\n",
        "    next_board, reward, done, info = model_play_turn(action)    # model plays a turn and we return the new state,\n",
        "                                                                # reward of the action picked, wether the game is\n",
        "                                                                # done, and other info\n",
        "\n",
        "    current_value = q_table[board, action] # get the \"old\" from the q-table so we can update it using q-learning\n",
        "    next_best_action_estimate = np.max(q_table[next_board]) # find the best action in the next state\n",
        "\n",
        "    new_value = (1 - learning_rate) * current_value + learning_rate * (reward + discount_factor * next_best_action_estimate) # from wikipedia article\n",
        "    q_table[board, action] = new_value\n",
        "\n",
        "    board = next_board\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 2. Deep Q-Learning\n",
        "\n",
        "# 3. Double Q-Learning"
      ],
      "metadata": {
        "id": "s90Hrtkt4sOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}